# Accelerated-Facial-Emotion-Recognition
This project tries to implement a CUDA-based architecture to improve performance and thus reduce 
latency in running inference during the feedforward propagation of a VGG19 model architecture-based 
Convolutional Neural Network(CNN). Here, a popular dataset called the Japanese female facial 
expression (JAFFE) dataset consisting of 213 images and 7 universal facial expressions is used for 
training the model using transfer learning and then performing inference on both Python(using Keras 
and TensorFlow backend) and CUDA and the objective is to make a comparison between the both.

# Problem Statement:
One drawback of CNN is that the training requires very large datasets to converge  to their global optima and the huge networks take a long time, even for inferencing. It is seen that most of the execution time of a convolutional layer is spent performing convolutions.5 Companies like Nvidia, Intel, Google, Qualcomm, AMD, and Microsoft have developed AI chips for either training a new network or inference on an already trained network, using the weights and biases values from training performed in the past. Super quick response time from these AI chips are going to be used for very critical applications such as self-driving cars. Thus, there is a need to speed up the inference to make these chips run faster. This creates the need to find ways to optimize the operations going on in these layers. To do so, we can leverage the decades of research done to optimize matrix-matrix multiplication. While the cuBLAS library has CUDA GEMM API and Intel MKL has special optimized CPU GEMM the hardware AI chips donâ€Ÿt have these libraries built into them. Hence the programmers need to perform all operations themselves and optimize the same.
This project aims to parallelize and thus speed up the feedforward of the VGG-19 architecture using a CUDA-based parallel computing architecture. This approach thus aims to efficiently exploit the GPU execution resources and in-core memories. Transfer learning for FEA has been chosen to achieve the same. Low latency and optimized inference can find many benefits in FEA devices used in critical industries like health and manufacturing. 
